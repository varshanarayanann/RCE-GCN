{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fbc440-5c6e-49e5-b056-5ad53b9fea0c",
   "metadata": {},
   "source": [
    "<b>Import required libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172429a-7e19-486b-a30b-dd499ad78473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries for data manipulation, modeling, and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580cfe47-33b9-43b9-a1d0-3d15f44d6ad5",
   "metadata": {},
   "source": [
    "<b>Define utility functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab26de-4f55-40c8-9bf8-7fc83608bdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for reproducibility and environment setup\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61559b52-b374-440a-a7f8-e675bdd1cc58",
   "metadata": {},
   "source": [
    "<b>Build graph structure from tabular data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfb149-ea4c-43a2-8895-953200e2f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build correlation-based adjacency matrix and feature matrix from a tabular dataset\n",
    "def build_sample_graph(data_df, feature_cols, label_col='label', threshold=0.7, scaler=None):\n",
    "    filtered_df = data_df[feature_cols + [label_col]]\n",
    "    labels = torch.tensor(filtered_df[label_col].values, dtype=torch.long)\n",
    "    features_df = filtered_df.drop(columns=[label_col])\n",
    "\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features_df.values)\n",
    "    else:\n",
    "        features = scaler.transform(features_df.values)\n",
    "\n",
    "    corr = np.corrcoef(features)\n",
    "    adj = (np.abs(corr) > threshold).astype(int)\n",
    "    np.fill_diagonal(adj, 0)\n",
    "    edge_index = np.array(np.nonzero(adj))\n",
    "    return torch.tensor(edge_index, dtype=torch.long), torch.tensor(features, dtype=torch.float), labels, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67e9a90-4dc4-4061-a4ac-9a7cdf4ad4ea",
   "metadata": {},
   "source": [
    "<b>Define the GCN model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a47fba-139a-4f7c-aea7-a9b50017a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple multi-layer Graph Convolutional Network for node classification\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels=64, dropout=0.4):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_classes)\n",
    "        self.dropout = dropout\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if hasattr(m, 'lin') and hasattr(m.lin, 'weight'):\n",
    "            torch.nn.init.xavier_uniform_(m.lin.weight)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545fa69-5559-4c0a-8300-667fa1669664",
   "metadata": {},
   "source": [
    "<b>Compute integrated gradients for feature importance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54e2d0-d4d4-4520-9c75-ad19e73d79ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate integrated gradients to estimate feature importance for model predictions\n",
    "def calculate_integrated_gradients(model, x, edge_index, target_label, num_steps=50):\n",
    "    model.eval()\n",
    "    baseline = torch.zeros_like(x)\n",
    "    importance_scores = torch.zeros_like(x)\n",
    "    for i in range(num_steps + 1):\n",
    "        alpha = i / num_steps\n",
    "        interpolated_input = baseline + alpha * (x - baseline)\n",
    "        interpolated_input.requires_grad = True\n",
    "        output = model(interpolated_input, edge_index)\n",
    "        target_output = output[:, target_label].sum()\n",
    "        model.zero_grad()\n",
    "        target_output.backward()\n",
    "        if interpolated_input.grad is not None:\n",
    "            importance_scores += interpolated_input.grad.detach()\n",
    "    integrated_gradients = (x - baseline) * importance_scores / num_steps\n",
    "    return torch.abs(integrated_gradients).sum(dim=0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d80d89e-b5ec-42ed-bdaa-9a098747f24a",
   "metadata": {},
   "source": [
    "<b>Recursive feature elimination using integrated gradients</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef5537-8934-4b50-8de7-3d8b9626089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively eliminate least important features based on integrated gradients \n",
    "def recursive_feature_elimination(data_df, device, label_col='label', hidden_channels=64, epochs=100,\n",
    "                                  elimination_rate=0.1, min_features_to_keep=None, seed=42):\n",
    "    set_seed(seed)\n",
    "    current_features = [c for c in data_df.columns if c != label_col]\n",
    "    best_features = current_features.copy()\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    if min_features_to_keep is None:\n",
    "        min_features_to_keep = 1\n",
    "    # User can change test_size to evaluate model performance.\n",
    "    train_val_df, _ = train_test_split(data_df, test_size=0.2, random_state=seed, stratify=data_df[label_col])\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=seed, stratify=train_val_df[label_col])\n",
    "\n",
    "    while len(current_features) > min_features_to_keep:\n",
    "        edge_train, x_train, y_train, scaler = build_sample_graph(train_df, current_features, label_col)\n",
    "        edge_val, x_val, y_val, _ = build_sample_graph(val_df, current_features, label_col, scaler=scaler)\n",
    "\n",
    "        num_features = x_train.shape[1]\n",
    "        num_classes = len(y_train.unique())\n",
    "\n",
    "        model = GCNModel(num_features, num_classes, hidden_channels).to(device)\n",
    "        y_np = y_train.cpu().numpy()\n",
    "        weights = compute_class_weight('balanced', classes=np.unique(y_np), y=y_np)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float32, device=device))\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "\n",
    "        # Train model\n",
    "        for _ in range(epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x_train.to(device), edge_train.to(device))\n",
    "            loss = criterion(out, y_train.to(device))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation accuracy\n",
    "        val_acc = evaluate_epoch(model, x_val.to(device), y_val.to(device), edge_val.to(device), criterion)[1]\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_features = current_features.copy()\n",
    "\n",
    "        # Compute IG and eliminate low-importance features\n",
    "        combined_scores = np.zeros(num_features)\n",
    "        for i in range(num_classes):\n",
    "            ig = calculate_integrated_gradients(model, x_train.to(device), edge_train.to(device), target_label=i)\n",
    "            combined_scores += np.abs(ig)\n",
    "\n",
    "        ranked = pd.DataFrame({'Feature': current_features, 'Score': combined_scores}).sort_values('Score', ascending=False)\n",
    "        remove_count = max(1, int(len(current_features) * elimination_rate))\n",
    "        current_features = ranked['Feature'][:-remove_count].tolist()\n",
    "\n",
    "    return best_val_acc, best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b950f51-6b06-4166-961e-4f03f8f248f6",
   "metadata": {},
   "source": [
    "<b>Evaluation and validation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b89f0-dd64-42d5-b594-4e22f6072736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance using accuracy and classification report\n",
    "def evaluate_epoch(model, x, y, edge_index, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        loss = criterion(out, y).item()\n",
    "        preds = out.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean().item()\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75647d4a-bc50-4db9-9eaa-81bb3474a972",
   "metadata": {},
   "source": [
    "<b>Main pipeline for dataset(s)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11028b79-15f4-41c6-8d34-867cb4925718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full pipeline on one or more datasets (The User can modify the parameters if they would like)\n",
    "def run_pipeline(file_paths, label_col='label', hidden_dim=64, epochs=100, elimination_rate=0.1, min_keep_ratio=0.05):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    seeds = [42, 100, 200, 300, 400]\n",
    "    val_seeds = [1, 2, 3, 4, 5]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(f\"\\nProcessing dataset: {os.path.basename(file_path)}\")\n",
    "        df = pd.read_csv(file_path, index_col=0)\n",
    "        min_features = max(1, int(df.drop(columns=[label_col]).shape[1] * min_keep_ratio))\n",
    "\n",
    "        all_acc, all_feats = [], []\n",
    "        for seed in seeds:\n",
    "            acc, feats = recursive_feature_elimination(df, device, label_col, hidden_dim, epochs, elimination_rate, min_features, seed)\n",
    "            all_acc.append(acc)\n",
    "            all_feats.append(feats)\n",
    "\n",
    "        print(f\"Best accuracy: {np.max(all_acc):.4f}, Avg accuracy: {np.mean(all_acc):.4f}\")\n",
    "        print(f\"Selected feature count: {len(all_feats[np.argmax(all_acc)])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f943d6dd-aff5-4b7e-b876-551dc00e706c",
   "metadata": {},
   "source": [
    "<b>Example usage</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373afb95-ef7f-46c0-9f4f-092b0bf9400f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage â€” replace with your own datasets (Make sure to place in correct file path)\n",
    "dataset_paths = [\n",
    "    \"data/sample_dataset1.csv\",\n",
    "    \"data/sample_dataset2.csv\"\n",
    "]\n",
    "\n",
    "run_pipeline(dataset_paths, label_col='label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
